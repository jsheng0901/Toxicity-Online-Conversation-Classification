{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "kernel0c324c70f2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "xTd3kxmTwkeR",
        "colab_type": "code",
        "colab": {},
        "outputId": "e7306f6b-bbdd-4157-dfa3-a2ea4ba35872"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n",
            "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
            "/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\n",
            "/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\n",
            "/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "N28ZhE8XwkeV",
        "colab_type": "code",
        "colab": {},
        "outputId": "bf376e4c-9511-47c7-a9ab-fa7b83022b53"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GooBIwPYwkeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILES = [\n",
        "    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n",
        "    '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
        "]\n",
        "NUM_MODELS = 2\n",
        "\n",
        "# the maximum number of different words to keep in the original texts\n",
        "# 100_000 seems good too\n",
        "MAX_FEATURES = 100000 \n",
        "\n",
        "#this is the number of training sample to put in theo model each step\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "#units parameters in Keras.layers.LSTM/cuDNNLSTM\n",
        "#it is the dimension of the output vector of each LSTM cell.\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "#we will convert each word in a comment_text to a number.\n",
        "#So a comment_text is a list of number. \n",
        "MAX_LEN = 220\n",
        "\n",
        "\n",
        "IDENTITY_COLUMNS = [\n",
        "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
        "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
        "]\n",
        "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
        "TEXT_COLUMN = 'comment_text'\n",
        "TARGET_COLUMN = 'target'\n",
        "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OrEbZ_Umwkeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "\n",
        "    with open(path) as f:\n",
        "        return dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BLXgA09Awked",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_matrix(word_index, path):\n",
        "\n",
        "    #we will construct an embedding_matrix for the words in word_index\n",
        "    #using pre-trained embedding word vectors from 'path'\n",
        "\n",
        "    embedding_index = load_embeddings(path)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "    # word_index is a dict. Each element is (word:i) where i is the index\n",
        "    # of the word\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            #RHS is a vector of 300d\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wqp5sVXUwkef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(embedding_matrix, num_aux_targets):\n",
        "    words = Input(shape=(MAX_LEN,))\n",
        "\n",
        "    #Embedding is the keras layer. We use the pre-trained embbeding_matrix\n",
        "    # x is a vector of 600 dimension\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "\n",
        "    #*embedding_matrix.shape is a short way for \n",
        "    #input_dim = embedding_matrix.shape[0], output_dim  = embedding_matrix.shape[1] output_dim is 600 for embing dim\n",
        "\n",
        "    #https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
        "    x = SpatialDropout1D(0.25)(x)\n",
        "\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    \n",
        "    # here is skip connection like resnet to overcome the vanishing gradient problem especially for deeper RNN \n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(1, activation='sigmoid', name = 'main_output')(hidden)\n",
        "\n",
        "    #num_aux_targets = 6 since y_aux_train has 6 columns, here activation use sigmoid becasue this is not muticlass\n",
        "    # just binary to each aux prediction \n",
        "    aux_result = Dense(num_aux_targets, activation='sigmoid', name = 'aux_ouput')(hidden)\n",
        "\n",
        "    model = Model(inputs=words, outputs=[result, aux_result])\n",
        "    # model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(clipnorm=0.1),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qH0Uq-q9wkeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
        "\n",
        "#Take the columns 'comment_text' from train,\n",
        "# then fillall NaN values by emtpy string '' (redundant)\n",
        "x_train = train[TEXT_COLUMN].fillna('').values\n",
        "\n",
        "#if true, y_train[i] =1, if false, it is = 0\n",
        "y_train = np.where(train[TARGET_COLUMN] >= 0.5, 1, 0)\n",
        "\n",
        "y_aux_train = train[AUX_COLUMNS]\n",
        "\n",
        "#Take the columns 'comment_text' from test,\n",
        "# then fillall NaN values by emtpy string '' (redundant)\n",
        "x_test = test[TEXT_COLUMN].fillna('').values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_9HIAHD1wkej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In here we don't use oov_token argument becasue we will tokenizer all train and test comment together. \n",
        "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
        "\n",
        "#we apply method fit_on_texts of tokenizer on x_train and x_test\n",
        "#it will initialize some parameters/attribute inside tokenizer\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
        "\n",
        "#tokenizer.index_word: the inverse of tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ALet5sFqwkel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we will convert each word in a comment_text to a number.\n",
        "#So a comment_text is a list of number corresponding to their original word \n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I0zrno01wkey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we want the length of this list is a constant -> MAX_LEN\n",
        "# if the list is longer, then we cut/trim it \n",
        "# if shorter, then we add/pad it with 0's at the beginning\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hCy7UYbGwke0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an embedding_matrix \n",
        "#after this, embedding_matrix is a matrix of size\n",
        "# len(tokenizer.word_index)+1   x 600\n",
        "# we concatenate two matrices, 600 = 300+300\n",
        "embedding_matrix = np.concatenate(\n",
        "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
        "#embedding_matrix.shape \n",
        "#== (410047, 600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eIZVBWlYwke5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_predictions = []\n",
        "weights = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UUZH2R36wkfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model_idx in range(NUM_MODELS):\n",
        "  # build the same models\n",
        "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
        "  # We train each model EPOCHS times\n",
        "  # After each epoch, we reset learning rate (we are using Adam Optimizer)  \n",
        "  # Summary model: build 2 same nlp model, in each model run 4 times fit and in each fit only run 1 time epoch. \n",
        "    for global_epoch in range(EPOCHS):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            [y_train, y_aux_train],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n",
        "            ]\n",
        "        )\n",
        "        #model.predict will give two outputs: main_output (target) and aux_output\n",
        "        #we only take main_output\n",
        "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
        "        weights.append(2 ** global_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1_edwqsqwkfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights is [1, 2, 4, 8, 1, 2, 4, 8]\n",
        "#predictions is an np.array, len is 97320 same as test \n",
        "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fWnXg2V2wkfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try more epochs\n",
        "model.fit(\n",
        "            x_train,\n",
        "            [y_train, y_aux_train],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=10,\n",
        "            verbose=1,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** epoch), verbose = 1)\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}